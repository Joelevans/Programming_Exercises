#Real Floating-Point Types

Real Types are the real numbers we know from math.
Rep by a floating-point and are float and double.

#Real Type Float
-32-bit
-AKA single precision real number.
-default value is 0.0f.
-f explicitly indicates that the number is of type float
(by default all real numbers are considered double)
-has accuracy up to 7 dp (the others are lost)
-range of values 
+-1.5 x 10^-45  to +-3.4 x 10^38.

##Special values of the Real Types
-special values that are not real numbers but mathematical
abstractions.
-Negative infinity(Single.NegativeInfinity)
Obtained when dividing -1.0f by 0.0f.

-Positive infinity(Single.PositiveInfinity)
Obtained when dividing 1.0f by 0.0f.

-Uncertainty(Single.NaN)
means that an invalid operation is performed by real numbers.
Obtained when e.g dividing 0.0f by 0.0f
as well as when calculating square root of a negative no.


#Real Type Double
-AKA double precision real number.
-64bit
-default value 0.0d
d isn't mandatory as all real numbers are double by default
- has a precision of 15-16 values
-ranges from +-5.0 x 10^-324 to +-1.7 x 10^308

smallest value is the constant Double.MinValue
= -1.79769e+308
and the largest value is Double.MaxValue
= 1.79769e+308

The closest to 0 is Double.Epsilon
=4.94066e-324

It can also take the special values
Double.PositiveInfinity, 
Double.NegativeInfinity
Double.NaN

#Errors in Calculations with Real Types
during representation of a real number, it tends to lose 
accuracy.
The reason of this is the inability of some real numbers
to be represented exactly as a sum of negative powers of
the number 2.

Examples of numbers that do not have an accurate rep
in float and double:
0.1, 1/3, 2/7 and other.
---FloatCalcError.cs--- [sample code]

Floating point number arithmetic can produce mistakes
and therefore not appropriate for precise financial
calculations.
C# supports decimal precision arithmetic where numbers like
0.1 are presented in the memory without rounding.


###Real Types with Decimal Precision
Where numbers are represented via the decimal numeral
system rather than the binary one. 
decimal floating-point arithmetic type does not lose accuracy
when storing and processing floating-point numbers.

decimal
-128-bit
-has a precision of 28 to 29 decimal places.
-minimal value is -7.9 x 10^28
-maximum value is +7.9 x 10^28
default value is 0.0m
m indicates that it is of type decimal

The closest to 0 are +-1.0 x 10 ^-28
It is obvious that decimal can store neither very big
numbers nor values very close to 0.
Though losses from rounding off are much smaller than when
using binary representation.
The main difference between real floating-point numbers and
real numbers with decimal precision is the accuracy of 
calculations and the extent to which they round up the
stored values. The double type allows us to work with 
very large values and values very close to zero but at 
the expense of accuracy and some unpleasant rounding errors.
The decimal type has smaller range but ensures greater 
accuracy in computation, as well as absence of anomalies 
with the decimal numbers.

All calculations with data of type decimal are done 
completely by software  rather than directly at low
microprocessor level, it is slower than double.
Only use when neccessary.
